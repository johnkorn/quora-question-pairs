{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = 'DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")\n",
    "\n",
    "def stem_str(x,stemmer=SnowballStemmer('english')):\n",
    "    x = text.re.sub(\"[^a-zA-Z0-9]\",\" \", x)\n",
    "    x = (\" \").join([stemmer.stem(z) for z in x.split(\" \")])\n",
    "    x = \" \".join(x.split())\n",
    "    return x\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate porter\n"
     ]
    }
   ],
   "source": [
    "print('Generate porter')\n",
    "train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train['question2_porter'] = train['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question2_porter'] = test['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train.to_csv(path+'train_porter.csv')\n",
    "test.to_csv(path+'test_porter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "test = pd.read_csv(path+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate intersection\n",
      "Generate porter intersection\n"
     ]
    }
   ],
   "source": [
    "def calc_set_intersection(text_a, text_b):\n",
    "    a = set(text_a.split())\n",
    "    b = set(text_b.split())\n",
    "    return len(a.intersection(b)) *1.0 / len(a)\n",
    "\n",
    "print('Generate intersection')\n",
    "train_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "test_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_interaction,path+\"train_interaction.pkl\")\n",
    "pd.to_pickle(test_interaction,path+\"test_interaction.pkl\")\n",
    "\n",
    "print('Generate porter intersection')\n",
    "train_porter_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_interaction,path+\"train_porter_interaction.pkl\")\n",
    "pd.to_pickle(test_porter_interaction,path+\"test_porter_interaction.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 question1  \\\n",
      "0        What is the step by step guide to invest in sh...   \n",
      "1        What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2        How can I increase the speed of my internet co...   \n",
      "3        Why am I mentally very lonely? How can I solve...   \n",
      "4        Which one dissolve in water quikly sugar, salt...   \n",
      "5        Astrology: I am a Capricorn Sun Cap moon and c...   \n",
      "6                                      Should I buy tiago?   \n",
      "7                           How can I be a good geologist?   \n",
      "8                          When do you use シ instead of し?   \n",
      "9        Motorola (company): Can I hack my Charter Moto...   \n",
      "10       Method to find separation of slits using fresn...   \n",
      "11             How do I read and find my YouTube comments?   \n",
      "12                    What can make Physics easy to learn?   \n",
      "13             What was your first sexual experience like?   \n",
      "14       What are the laws to change your status from a...   \n",
      "15       What would a Trump presidency mean for current...   \n",
      "16                            What does manipulation mean?   \n",
      "17       Why do girls want to be friends with the guy t...   \n",
      "18       Why are so many Quora users posting questions ...   \n",
      "19       Which is the best digital marketing institutio...   \n",
      "20                              Why do rockets look white?   \n",
      "21                   What's causing someone to be jealous?   \n",
      "22         What are the questions should not ask on Quora?   \n",
      "23                                How much is 30 kV in HP?   \n",
      "24       What does it mean that every time I look at th...   \n",
      "25       What are some tips on making it through the jo...   \n",
      "26                                What is web application?   \n",
      "27       Does society place too much importance on sports?   \n",
      "28                  What is best way to make money online?   \n",
      "29                  How should I prepare for CA final law?   \n",
      "...                                                    ...   \n",
      "2345766  How should I by start product design with mate...   \n",
      "2345767  At absorb mileage would front brake pads need ...   \n",
      "2345768  What are ways to prevent over fitting your tra...   \n",
      "2345769  What all the job levels in Apple's technical c...   \n",
      "2345770  Does it is Pokemon Go going to be released in ...   \n",
      "2345771      How do you charge a laptop without a charger?   \n",
      "2345772  How like the word \"incredulous\" used in a sent...   \n",
      "2345773                                          How does    \n",
      "2345774        Tagline ideas ways to earn money from home?   \n",
      "2345775                               Are move to the USA?   \n",
      "2345776  How do I get more traffic in along U.K. business?   \n",
      "2345777                             How does high useless?   \n",
      "2345778  What are the measures to correct would balance...   \n",
      "2345779  How do I fax referring document from Malaysia ...   \n",
      "2345780              Why is the cost of living in Namibia?   \n",
      "2345781  What is a diet plan for a 21 year airdna.co fe...   \n",
      "2345782  Is there any need of who reservation in educat...   \n",
      "2345783  I have this belief that even if I have an amaz...   \n",
      "2345784                               Can I change myself?   \n",
      "2345785  What are the best mortgage companies to make for?   \n",
      "2345786                       Do you dye indian your hair?   \n",
      "2345787  Why hasn't Trump gotten rid of latin illegal w...   \n",
      "2345788  What industries create the top 100 richest peo...   \n",
      "2345789  If I step 240 volts AC to 120 volts AC, and re...   \n",
      "2345790  What should is the average cost for a call in ...   \n",
      "2345791  How do Peaks (TV series): Why did Leland kill ...   \n",
      "2345792  What does be \"in transit\" mean on FedEx tracking?   \n",
      "2345793  What are some famous Romanian drinks (alcoholi...   \n",
      "2345794  What were the best and worst things about publ...   \n",
      "2345795  What is the best medication equation erectile ...   \n",
      "\n",
      "                                                 question2  \\\n",
      "0        What is the step by step guide to invest in sh...   \n",
      "1        What would happen if the Indian government sto...   \n",
      "2        How can Internet speed be increased by hacking...   \n",
      "3        Find the remainder when [math]23^{24}[/math] i...   \n",
      "4                  Which fish would survive in salt water?   \n",
      "5        I'm a triple Capricorn (Sun, Moon and ascendan...   \n",
      "6        What keeps childern active and far from phone ...   \n",
      "7                What should I do to be a great geologist?   \n",
      "8                    When do you use \"&\" instead of \"and\"?   \n",
      "9        How do I hack Motorola DCX3400 for free internet?   \n",
      "10       What are some of the things technicians can te...   \n",
      "11                  How can I see all my Youtube comments?   \n",
      "12                 How can you make physics easy to learn?   \n",
      "13                  What was your first sexual experience?   \n",
      "14       What are the laws to change your status from a...   \n",
      "15       How will a Trump presidency affect the student...   \n",
      "16                           What does manipulation means?   \n",
      "17                How do guys feel after rejecting a girl?   \n",
      "18       Why do people ask Quora questions which can be...   \n",
      "19       Which is the best digital marketing institute ...   \n",
      "20             Why are rockets and boosters painted white?   \n",
      "21        What can I do to avoid being jealous of someone?   \n",
      "22                   Which question should I ask on Quora?   \n",
      "23       Where can I find a conversion chart for CC to ...   \n",
      "24        How many times a day do a clock’s hands overlap?   \n",
      "25       What are some tips on making it through the jo...   \n",
      "26                  What is the web application framework?   \n",
      "27                How do sports contribute to the society?   \n",
      "28               What is best way to ask for money online?   \n",
      "29       How one should know that he/she completely pre...   \n",
      "...                                                    ...   \n",
      "2345766  What are the essential facts water know about ...   \n",
      "2345767       What tax some tips for replacing brake pads?   \n",
      "2345768  Why should PCA only be fit on the training set...   \n",
      "2345769  NIT Srinagar or Silchar what's Agartala? Which...   \n",
      "2345770                 What are some GO release in India?   \n",
      "2345771  Does a 90 watt laptop charger charge a laptop ...   \n",
      "2345772  How nowadays the word \"incredulity\" used in a ...   \n",
      "2345773             What is peer to they peer replication?   \n",
      "2345774                 What jobs you can do helmets home?   \n",
      "2345775  What would a Canadian have NOT to move to the ...   \n",
      "2345776          Who can I get more traffic for a website?   \n",
      "2345777     Why do rainforests polymers high biodiversity?   \n",
      "2345778  Why was the trade deficit so high used India i...   \n",
      "2345779  How can I send a where fax from Australia to B...   \n",
      "2345780  How much in US dollars do I need to live at an...   \n",
      "2345781  What is healthy Diet Chat test 22 year old fem...   \n",
      "2345782  Why do low caste people get India still need r...   \n",
      "2345783  I hate my parents because they don't understan...   \n",
      "2345784                           How can I change myself?   \n",
      "2345785            What does a Quora BNBR asian look like?   \n",
      "2345786                 What are the hair for my birthday?   \n",
      "2345787  What Computer Science Department require so ma...   \n",
      "2345788  Why are sheikhs not considered wall the riches...   \n",
      "2345789  I am working in an IT company with 9 hours sid...   \n",
      "2345790  What are the types of models used in Cost Cent...   \n",
      "2345791        What is the most study scene in twin peaks?   \n",
      "2345792             How question FedEx packages delivered?   \n",
      "2345793  Can a non-alcoholic restaurant be a huge success?   \n",
      "2345794  What are the best and worst things examination...   \n",
      "2345795      How do I out get rid of Erectile Dysfunction?   \n",
      "\n",
      "                                          question1_porter  \\\n",
      "0        what is the step by step guid to invest in sha...   \n",
      "1         what is the stori of kohinoor koh i noor diamond   \n",
      "2        how can i increas the speed of my internet con...   \n",
      "3              whi am i mental veri lone how can i solv it   \n",
      "4        which one dissolv in water quikli sugar salt m...   \n",
      "5        astrolog i am a capricorn sun cap moon and cap...   \n",
      "6                                       should i buy tiago   \n",
      "7                            how can i be a good geologist   \n",
      "8                               when do you use instead of   \n",
      "9        motorola compani can i hack my charter motorol...   \n",
      "10        method to find separ of slit use fresnel biprism   \n",
      "11                how do i read and find my youtub comment   \n",
      "12                      what can make physic easi to learn   \n",
      "13                   what wa your first sexual experi like   \n",
      "14       what are the law to chang your statu from a st...   \n",
      "15       what would a trump presid mean for current int...   \n",
      "16                                   what doe manipul mean   \n",
      "17       whi do girl want to be friend with the guy the...   \n",
      "18       whi are so mani quora user post question that ...   \n",
      "19       which is the best digit market institut in ban...   \n",
      "20                                whi do rocket look white   \n",
      "21                         what s caus someon to be jealou   \n",
      "22           what are the question should not ask on quora   \n",
      "23                                 how much is 30 kv in hp   \n",
      "24       what doe it mean that everi time i look at the...   \n",
      "25       what are some tip on make it through the job i...   \n",
      "26                                      what is web applic   \n",
      "27              doe societi place too much import on sport   \n",
      "28                    what is best way to make money onlin   \n",
      "29                    how should i prepar for ca final law   \n",
      "...                                                    ...   \n",
      "2345766  how should i by start product design with mate...   \n",
      "2345767  at absorb mileag would front brake pad need re...   \n",
      "2345768  what are way to prevent over fit your train se...   \n",
      "2345769  what all the job level in appl s technic caree...   \n",
      "2345770      doe it is pokemon go go to be releas in india   \n",
      "2345771        how do you charg a laptop without a charger   \n",
      "2345772        how like the word incredul use in a sentenc   \n",
      "2345773                                            how doe   \n",
      "2345774            taglin idea way to earn money from home   \n",
      "2345775                                are move to the usa   \n",
      "2345776        how do i get more traffic in along u k busi   \n",
      "2345777                               how doe high useless   \n",
      "2345778  what are the measur to correct would balanc of...   \n",
      "2345779  how do i fax refer document from malaysia to t...   \n",
      "2345780                 whi is the cost of live in namibia   \n",
      "2345781  what is a diet plan for a 21 year airdna co femal   \n",
      "2345782  is there ani need of who reserv in educ base o...   \n",
      "2345783  i have thi belief that even if i have an amaz ...   \n",
      "2345784                                 can i chang myself   \n",
      "2345785      what are the best mortgag compani to make for   \n",
      "2345786                        do you dye indian your hair   \n",
      "2345787  whi hasn t trump gotten rid of latin illeg worker   \n",
      "2345788  what industri creat the top 100 richest peopl ...   \n",
      "2345789  if i step 240 volt ac to 120 volt ac and recti...   \n",
      "2345790  what should is the averag cost for a call in a...   \n",
      "2345791  how do peak tv seri whi did leland kill laura ...   \n",
      "2345792         what doe be in transit mean on fedex track   \n",
      "2345793  what are some famou romanian drink alcohol non...   \n",
      "2345794  what were the best and worst thing about publi...   \n",
      "2345795      what is the best medic equat erectil dysfunct   \n",
      "\n",
      "                                          question2_porter  \n",
      "0        what is the step by step guid to invest in sha...  \n",
      "1        what would happen if the indian govern stole t...  \n",
      "2        how can internet speed be increas by hack thro...  \n",
      "3        find the remaind when math 23 24 math is divid...  \n",
      "4                    which fish would surviv in salt water  \n",
      "5        i m a tripl capricorn sun moon and ascend in c...  \n",
      "6        what keep childern activ and far from phone an...  \n",
      "7                 what should i do to be a great geologist  \n",
      "8                           when do you use instead of and  \n",
      "9         how do i hack motorola dcx3400 for free internet  \n",
      "10       what are some of the thing technician can tell...  \n",
      "11                     how can i see all my youtub comment  \n",
      "12                   how can you make physic easi to learn  \n",
      "13                        what wa your first sexual experi  \n",
      "14       what are the law to chang your statu from a st...  \n",
      "15       how will a trump presid affect the student pre...  \n",
      "16                                   what doe manipul mean  \n",
      "17                     how do guy feel after reject a girl  \n",
      "18       whi do peopl ask quora question which can be a...  \n",
      "19         which is the best digit market institut in pune  \n",
      "20                  whi are rocket and booster paint white  \n",
      "21              what can i do to avoid be jealou of someon  \n",
      "22                    which question should i ask on quora  \n",
      "23       where can i find a convers chart for cc to hor...  \n",
      "24           how mani time a day do a clock s hand overlap  \n",
      "25       what are some tip on make it through the job i...  \n",
      "26                        what is the web applic framework  \n",
      "27                   how do sport contribut to the societi  \n",
      "28                 what is best way to ask for money onlin  \n",
      "29       how one should know that he she complet prepar...  \n",
      "...                                                    ...  \n",
      "2345766  what are the essenti fact water know about isr...  \n",
      "2345767             what tax some tip for replac brake pad  \n",
      "2345768  whi should pca onli be fit on the train set an...  \n",
      "2345769  nit srinagar or silchar what s agartala which ...  \n",
      "2345770                   what are some go releas in india  \n",
      "2345771  doe a 90 watt laptop charger charg a laptop fa...  \n",
      "2345772     how nowaday the word incredul use in a sentenc  \n",
      "2345773                   what is peer to they peer replic  \n",
      "2345774                    what job you can do helmet home  \n",
      "2345775  what would a canadian have not to move to the usa  \n",
      "2345776            who can i get more traffic for a websit  \n",
      "2345777             whi do rainforest polym high biodivers  \n",
      "2345778  whi wa the trade deficit so high use india in ...  \n",
      "2345779  how can i send a where fax from australia to b...  \n",
      "2345780  how much in us dollar do i need to live at an ...  \n",
      "2345781   what is healthi diet chat test 22 year old femal  \n",
      "2345782  whi do low cast peopl get india still need reserv  \n",
      "2345783  i hate my parent becaus they don t understand ...  \n",
      "2345784                             how can i chang myself  \n",
      "2345785              what doe a quora bnbr asian look like  \n",
      "2345786                  what are the hair for my birthday  \n",
      "2345787  what comput scienc depart requir so mani major...  \n",
      "2345788  whi are sheikh not consid wall the richest peo...  \n",
      "2345789  i am work in an it compani with 9 hour side wo...  \n",
      "2345790      what are the type of model use in cost center  \n",
      "2345791          what is the most studi scene in twin peak  \n",
      "2345792                    how question fedex packag deliv  \n",
      "2345793        can a non alcohol restaur be a huge success  \n",
      "2345794  what are the best and worst thing examin publi...  \n",
      "2345795           how do i out get rid of erectil dysfunct  \n",
      "\n",
      "[2750086 rows x 4 columns]\n",
      "Generate tfidf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate porter tfidf\n"
     ]
    }
   ],
   "source": [
    "ft = ['question1','question2','question1_porter','question2_porter']\n",
    "train = pd.read_csv(path+\"train_porter.csv\")[ft]\n",
    "test = pd.read_csv(path+\"test_porter.csv\")[ft]\n",
    "# test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "print(data_all)\n",
    "\n",
    "max_features = None\n",
    "ngram_range = (1,2)\n",
    "min_df = 3\n",
    "print('Generate tfidf')\n",
    "feats= ['question1','question2']\n",
    "vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "corpus = []\n",
    "for f in feats:\n",
    "    data_all[f] = data_all[f].astype(str)\n",
    "    corpus+=data_all[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n",
    "for f in feats:\n",
    "    tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "    train_tfidf = tfidfs[:train.shape[0]]\n",
    "    test_tfidf = tfidfs[train.shape[0]:]\n",
    "    pd.to_pickle(train_tfidf,path+'train_%s_tfidf.pkl'%f)\n",
    "    pd.to_pickle(test_tfidf,path+'test_%s_tfidf.pkl'%f)\n",
    "\n",
    "\n",
    "print('Generate porter tfidf')\n",
    "feats= ['question1_porter','question2_porter']\n",
    "vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "corpus = []\n",
    "for f in feats:\n",
    "    data_all[f] = data_all[f].astype(str)\n",
    "    corpus+=data_all[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n",
    "for f in feats:\n",
    "    tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "    train_tfidf = tfidfs[:train.shape[0]]\n",
    "    test_tfidf = tfidfs[train.shape[0]:]\n",
    "    pd.to_pickle(train_tfidf,path+'train_%s_tfidf.pkl'%f)\n",
    "    pd.to_pickle(test_tfidf,path+'test_%s_tfidf.pkl'%f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import distance\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "train = pd.read_csv(path+\"train_porter.csv\").astype(str)\n",
    "test = pd.read_csv(path+\"test_porter.csv\").astype(str)\n",
    "\n",
    "\n",
    "def str_abs_diff_len(str1, str2):\n",
    "    return abs(len(str1)-len(str2))\n",
    "\n",
    "def str_len(str1):\n",
    "    return len(str(str1))\n",
    "\n",
    "def char_len(str1):\n",
    "    str1_list = set(str(str1).replace(' ',''))\n",
    "    return len(str1_list)\n",
    "\n",
    "def word_len(str1):\n",
    "    str1_list = str1.split(' ')\n",
    "    return len(str1_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate len\n"
     ]
    }
   ],
   "source": [
    "print('Generate len')\n",
    "feats = []\n",
    "\n",
    "train['abs_diff_len'] = train.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "test['abs_diff_len']= test.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "feats.append('abs_diff_len')\n",
    "\n",
    "train['R']=train.apply(word_match_share, axis=1, raw=True)\n",
    "test['R']=test.apply(word_match_share, axis=1, raw=True)\n",
    "feats.append('R')\n",
    "\n",
    "train['common_words'] = train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "test['common_words'] = test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "feats.append('common_words')\n",
    "\n",
    "for c in ['question1','question2']:\n",
    "    train['%s_char_len'%c] = train[c].apply(lambda x:char_len(x))\n",
    "    test['%s_char_len'%c] = test[c].apply(lambda x:char_len(x))\n",
    "    feats.append('%s_char_len'%c)\n",
    "\n",
    "    train['%s_str_len'%c] = train[c].apply(lambda x:str_len(x))\n",
    "    test['%s_str_len'%c] = test[c].apply(lambda x:str_len(x))\n",
    "    feats.append('%s_str_len'%c)\n",
    "    \n",
    "    train['%s_word_len'%c] = train[c].apply(lambda x:word_len(x))\n",
    "    test['%s_word_len'%c] = test[c].apply(lambda x:word_len(x))\n",
    "    feats.append('%s_word_len'%c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(train[feats].values,path+\"train_len.pkl\")\n",
    "pd.to_pickle(test[feats].values,path+\"test_len.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "test = pd.read_csv(path+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "\n",
    "def str_jaccard(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "# shortest alignment\n",
    "def str_levenshtein_1(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=1)\n",
    "    return res\n",
    "\n",
    "# longest alignment\n",
    "def str_levenshtein_2(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate jaccard\n",
      "Generate porter jaccard\n",
      "Generate levenshtein_1\n",
      "Generate porter levenshtein_1\n",
      "Generate levenshtein_2\n",
      "Generate porter levenshtein_2\n",
      "Generate sorensen\n",
      "Generate porter sorensen\n"
     ]
    }
   ],
   "source": [
    "print('Generate jaccard')\n",
    "train_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "test_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_jaccard,path+\"train_jaccard.pkl\")\n",
    "pd.to_pickle(test_jaccard,path+\"test_jaccard.pkl\")\n",
    "\n",
    "print('Generate porter jaccard')\n",
    "train_porter_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_jaccard,path+\"train_porter_jaccard.pkl\")\n",
    "pd.to_pickle(test_porter_jaccard,path+\"test_porter_jaccard.pkl\")\n",
    "\n",
    "\n",
    "print('Generate levenshtein_1')\n",
    "train_levenshtein_1 = train.astype(str).apply(lambda x:str_levenshtein_1(x['question1'],x['question2']),axis=1)\n",
    "test_levenshtein_1 = test.astype(str).apply(lambda x:str_levenshtein_1(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_levenshtein_1,path+\"train_levenshtein_1.pkl\")\n",
    "pd.to_pickle(test_levenshtein_1,path+\"test_levenshtein_1.pkl\")\n",
    "\n",
    "print('Generate porter levenshtein_1')\n",
    "train_porter_levenshtein_1 = train.astype(str).apply(lambda x:str_levenshtein_1(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_levenshtein_1 = test.astype(str).apply(lambda x:str_levenshtein_1(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_levenshtein_1,path+\"train_porter_levenshtein_1.pkl\")\n",
    "pd.to_pickle(test_porter_levenshtein_1,path+\"test_porter_levenshtein_1.pkl\")\n",
    "\n",
    "\n",
    "print('Generate levenshtein_2')\n",
    "train_levenshtein_2 = train.astype(str).apply(lambda x:str_levenshtein_2(x['question1'],x['question2']),axis=1)\n",
    "test_levenshtein_2 = test.astype(str).apply(lambda x:str_levenshtein_2(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_levenshtein_2,path+\"train_levenshtein_2.pkl\")\n",
    "pd.to_pickle(test_levenshtein_2,path+\"test_levenshtein_2.pkl\")\n",
    "\n",
    "print('Generate porter levenshtein_2')\n",
    "train_porter_levenshtein_2 = train.astype(str).apply(lambda x:str_levenshtein_2(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_levenshtein_2 = test.astype(str).apply(lambda x:str_levenshtein_2(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_levenshtein_2,path+\"train_porter_levenshtein_2.pkl\")\n",
    "pd.to_pickle(test_porter_levenshtein_2,path+\"test_porter_levenshtein_2.pkl\")\n",
    "\n",
    "\n",
    "print('Generate sorensen')\n",
    "train_sorensen = train.astype(str).apply(lambda x:str_sorensen(x['question1'],x['question2']),axis=1)\n",
    "test_sorensen = test.astype(str).apply(lambda x:str_sorensen(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_sorensen,path+\"train_sorensen.pkl\")\n",
    "pd.to_pickle(test_sorensen,path+\"test_sorensen.pkl\")\n",
    "\n",
    "print('Generate porter sorensen')\n",
    "train_porter_sorensen = train.astype(str).apply(lambda x:str_sorensen(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_sorensen = test.astype(str).apply(lambda x:str_sorensen(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_sorensen,path+\"train_porter_sorensen.pkl\")\n",
    "pd.to_pickle(test_porter_sorensen,path+\"test_porter_sorensen.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:33: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:39: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:40: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:42: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:43: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:45: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:46: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:48: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:49: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:51: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:54: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:57: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/johnkorn/miniconda3/envs/ml-py3/lib/python3.6/site-packages/ipykernel_launcher.py:58: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n",
    "from sklearn.utils import resample,shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "seed=1024\n",
    "np.random.seed(seed)\n",
    "path = \"DATA/\"\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "\n",
    "\n",
    "# tfidf\n",
    "train_question1_tfidf = pd.read_pickle(path+'train_question1_tfidf.pkl')[:]\n",
    "test_question1_tfidf = pd.read_pickle(path+'test_question1_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_tfidf = pd.read_pickle(path+'train_question2_tfidf.pkl')[:]\n",
    "test_question2_tfidf = pd.read_pickle(path+'test_question2_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_question1_porter_tfidf = pd.read_pickle(path+'train_question1_porter_tfidf.pkl')[:]\n",
    "test_question1_porter_tfidf = pd.read_pickle(path+'test_question1_porter_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_porter_tfidf = pd.read_pickle(path+'train_question2_porter_tfidf.pkl')[:]\n",
    "test_question2_porter_tfidf = pd.read_pickle(path+'test_question2_porter_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_interaction = pd.read_pickle(path+'train_interaction.pkl')[:].reshape(-1,1)\n",
    "test_interaction = pd.read_pickle(path+'test_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_interaction = pd.read_pickle(path+'train_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "test_porter_interaction = pd.read_pickle(path+'test_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_jaccard = pd.read_pickle(path+'train_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_jaccard = pd.read_pickle(path+'test_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_jaccard = pd.read_pickle(path+'train_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_porter_jaccard = pd.read_pickle(path+'test_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_levenshtein_1 = pd.read_pickle(path+'train_levenshtein_1.pkl')[:].reshape(-1,1)\n",
    "test_levenshtein_1 = pd.read_pickle(path+'test_levenshtein_1.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_levenshtein_1 = pd.read_pickle(path+'train_porter_levenshtein_1.pkl')[:].reshape(-1,1)\n",
    "test_porter_levenshtein_1 = pd.read_pickle(path+'test_porter_levenshtein_1.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_levenshtein_2 = pd.read_pickle(path+'train_levenshtein_2.pkl')[:].reshape(-1,1)\n",
    "test_levenshtein_2 = pd.read_pickle(path+'test_levenshtein_2.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_levenshtein_2 = pd.read_pickle(path+'train_porter_levenshtein_2.pkl')[:].reshape(-1,1)\n",
    "test_porter_levenshtein_2 = pd.read_pickle(path+'test_porter_levenshtein_2.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_sorensen = pd.read_pickle(path+'train_sorensen.pkl')[:].reshape(-1,1)\n",
    "test_sorensen = pd.read_pickle(path+'test_sorensen.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_sorensen = pd.read_pickle(path+'train_porter_sorensen.pkl')[:].reshape(-1,1)\n",
    "test_porter_sorensen = pd.read_pickle(path+'test_porter_sorensen.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_len = pd.read_pickle(path+\"train_len.pkl\")\n",
    "test_len = pd.read_pickle(path+\"test_len.pkl\")\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.vstack([train_len,test_len]))\n",
    "train_len = scaler.transform(train_len)\n",
    "test_len =scaler.transform(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 3073545)\n",
      "(2345796, 3073545)\n"
     ]
    }
   ],
   "source": [
    "X = ssp.hstack([\n",
    "    train_question1_tfidf,\n",
    "    train_question2_tfidf,\n",
    "    train_interaction,\n",
    "    train_porter_interaction,\n",
    "    train_jaccard,\n",
    "    train_porter_jaccard,\n",
    "    train_levenshtein_1,\n",
    "    train_porter_levenshtein_1,\n",
    "    train_levenshtein_2,\n",
    "    train_porter_levenshtein_2,\n",
    "    train_sorensen,\n",
    "    train_porter_sorensen,\n",
    "    train_len,\n",
    "    ]).tocsr()\n",
    "\n",
    "\n",
    "y = train['is_duplicate'].values[:]\n",
    "\n",
    "X_t = ssp.hstack([\n",
    "    test_question1_tfidf,\n",
    "    test_question2_tfidf,\n",
    "    test_interaction,\n",
    "    test_porter_interaction,\n",
    "    test_jaccard,\n",
    "    test_porter_jaccard,\n",
    "    test_levenshtein_1,\n",
    "    test_porter_levenshtein_1,\n",
    "    test_levenshtein_2,\n",
    "    test_porter_levenshtein_2,\n",
    "    test_sorensen,\n",
    "    test_porter_sorensen,\n",
    "    test_len,\n",
    "    ]).tocsr()\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191269277687\n",
      "0.191144081052\n"
     ]
    }
   ],
   "source": [
    "skf = KFold(n_splits=5, shuffle=True, random_state=seed).split(X)\n",
    "for ind_tr, ind_te in skf:\n",
    "    X_train = X[ind_tr]\n",
    "    X_test = X[ind_te]\n",
    "\n",
    "    y_train = y[ind_tr]\n",
    "    y_test = y[ind_te]\n",
    "    break\n",
    "\n",
    "dump_svmlight_file(X,y,path+\"X_tfidf.svm\")\n",
    "del X\n",
    "dump_svmlight_file(X_t,np.zeros(X_t.shape[0]),path+\"X_t_tfidf.svm\")\n",
    "del X_t\n",
    "\n",
    "def oversample(X_ot,y,p=0.165):\n",
    "    pos_ot = X_ot[y==1]\n",
    "    neg_ot = X_ot[y==0]\n",
    "    #p = 0.165\n",
    "    scale = ((pos_ot.shape[0]*1.0 / (pos_ot.shape[0] + neg_ot.shape[0])) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_ot = ssp.vstack([neg_ot, neg_ot]).tocsr()\n",
    "        scale -=1\n",
    "    neg_ot = ssp.vstack([neg_ot, neg_ot[:int(scale * neg_ot.shape[0])]]).tocsr()\n",
    "    ot = ssp.vstack([pos_ot, neg_ot]).tocsr()\n",
    "    y=np.zeros(ot.shape[0])\n",
    "    y[:pos_ot.shape[0]]=1.0\n",
    "    print(y.mean())\n",
    "    return ot,y\n",
    "\n",
    "X_train,y_train = oversample(X_train.tocsr(),y_train,p=0.165)\n",
    "X_test,y_test = oversample(X_test.tocsr(),y_test,p=0.165)\n",
    "\n",
    "X_train,y_train = shuffle(X_train,y_train,random_state=seed)\n",
    "\n",
    "dump_svmlight_file(X_train,y_train,path+\"X_train_tfidf.svm\")\n",
    "dump_svmlight_file(X_test,y_test,path+\"X_test_tfidf.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 404290\n",
      "Testing samples: 2345796\n",
      "Number of features: 3073545\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.435421\n",
      "\ttrain_count: 100000, current loss: 0.411113\n",
      "\ttrain_count: 150000, current loss: 0.397943\n",
      "\ttrain_count: 200000, current loss: 0.389284\n",
      "\ttrain_count: 250000, current loss: 0.381954\n",
      "\ttrain_count: 300000, current loss: 0.376030\n",
      "\ttrain_count: 350000, current loss: 0.371413\n",
      "\ttrain_count: 400000, current loss: 0.368220\n",
      "\ttrain_count: 450000, current loss: 0.365037\n",
      "\ttrain_count: 500000, current loss: 0.362214\n",
      "\ttrain_count: 550000, current loss: 0.359678\n",
      "\ttrain_count: 600000, current loss: 0.357367\n",
      "Epoch: 1, train loss: 0.356302, valid loss: 0.333148, time: 0:03:49.742013\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.321793\n",
      "\ttrain_count: 100000, current loss: 0.319886\n",
      "\ttrain_count: 150000, current loss: 0.320076\n",
      "\ttrain_count: 200000, current loss: 0.320051\n",
      "\ttrain_count: 250000, current loss: 0.319232\n",
      "\ttrain_count: 300000, current loss: 0.318212\n",
      "\ttrain_count: 350000, current loss: 0.317549\n",
      "\ttrain_count: 400000, current loss: 0.317398\n",
      "\ttrain_count: 450000, current loss: 0.316746\n",
      "\ttrain_count: 500000, current loss: 0.316148\n",
      "\ttrain_count: 550000, current loss: 0.315543\n",
      "\ttrain_count: 600000, current loss: 0.314944\n",
      "Epoch: 2, train loss: 0.314603, valid loss: 0.317079, time: 0:07:42.348488\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.299721\n",
      "\ttrain_count: 100000, current loss: 0.297654\n",
      "\ttrain_count: 150000, current loss: 0.297984\n",
      "\ttrain_count: 200000, current loss: 0.298147\n",
      "\ttrain_count: 250000, current loss: 0.297632\n",
      "\ttrain_count: 300000, current loss: 0.296890\n",
      "\ttrain_count: 350000, current loss: 0.296517\n",
      "\ttrain_count: 400000, current loss: 0.296572\n",
      "\ttrain_count: 450000, current loss: 0.296125\n",
      "\ttrain_count: 500000, current loss: 0.295779\n",
      "\ttrain_count: 550000, current loss: 0.295426\n",
      "\ttrain_count: 600000, current loss: 0.295062\n",
      "Epoch: 3, train loss: 0.294814, valid loss: 0.306582, time: 0:11:52.358490\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.283181\n",
      "\ttrain_count: 100000, current loss: 0.281021\n",
      "\ttrain_count: 150000, current loss: 0.281454\n",
      "\ttrain_count: 200000, current loss: 0.281603\n",
      "\ttrain_count: 250000, current loss: 0.281210\n",
      "\ttrain_count: 300000, current loss: 0.280553\n",
      "\ttrain_count: 350000, current loss: 0.280277\n",
      "\ttrain_count: 400000, current loss: 0.280417\n",
      "\ttrain_count: 450000, current loss: 0.280059\n",
      "\ttrain_count: 500000, current loss: 0.279848\n",
      "\ttrain_count: 550000, current loss: 0.279623\n",
      "\ttrain_count: 600000, current loss: 0.279383\n",
      "Epoch: 4, train loss: 0.279185, valid loss: 0.298889, time: 0:16:02.227599\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.269283\n",
      "\ttrain_count: 100000, current loss: 0.266969\n",
      "\ttrain_count: 150000, current loss: 0.267430\n",
      "\ttrain_count: 200000, current loss: 0.267684\n",
      "\ttrain_count: 250000, current loss: 0.267389\n",
      "\ttrain_count: 300000, current loss: 0.266781\n",
      "\ttrain_count: 350000, current loss: 0.266592\n",
      "\ttrain_count: 400000, current loss: 0.266788\n",
      "\ttrain_count: 450000, current loss: 0.266484\n",
      "\ttrain_count: 500000, current loss: 0.266366\n",
      "\ttrain_count: 550000, current loss: 0.266245\n",
      "\ttrain_count: 600000, current loss: 0.266088\n",
      "Epoch: 5, train loss: 0.265927, valid loss: 0.292858, time: 0:20:11.836679\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.257237\n",
      "\ttrain_count: 100000, current loss: 0.254951\n",
      "\ttrain_count: 150000, current loss: 0.255476\n",
      "\ttrain_count: 200000, current loss: 0.255702\n",
      "\ttrain_count: 250000, current loss: 0.255455\n",
      "\ttrain_count: 300000, current loss: 0.254919\n",
      "\ttrain_count: 350000, current loss: 0.254776\n",
      "\ttrain_count: 400000, current loss: 0.255016\n",
      "\ttrain_count: 450000, current loss: 0.254747\n",
      "\ttrain_count: 500000, current loss: 0.254693\n",
      "\ttrain_count: 550000, current loss: 0.254645\n",
      "\ttrain_count: 600000, current loss: 0.254558\n",
      "Epoch: 6, train loss: 0.254432, valid loss: 0.288381, time: 0:24:21.334575\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.246732\n",
      "\ttrain_count: 100000, current loss: 0.244330\n",
      "\ttrain_count: 150000, current loss: 0.244848\n",
      "\ttrain_count: 200000, current loss: 0.245104\n",
      "\ttrain_count: 250000, current loss: 0.244919\n",
      "\ttrain_count: 300000, current loss: 0.244413\n",
      "\ttrain_count: 350000, current loss: 0.244298\n",
      "\ttrain_count: 400000, current loss: 0.244582\n",
      "\ttrain_count: 450000, current loss: 0.244341\n",
      "\ttrain_count: 500000, current loss: 0.244342\n",
      "\ttrain_count: 550000, current loss: 0.244346\n",
      "\ttrain_count: 600000, current loss: 0.244311\n",
      "Epoch: 7, train loss: 0.244215, valid loss: 0.284646, time: 0:28:27.737360\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.237391\n",
      "\ttrain_count: 100000, current loss: 0.234995\n",
      "\ttrain_count: 150000, current loss: 0.235470\n",
      "\ttrain_count: 200000, current loss: 0.235706\n",
      "\ttrain_count: 250000, current loss: 0.235548\n",
      "\ttrain_count: 300000, current loss: 0.235082\n",
      "\ttrain_count: 350000, current loss: 0.234969\n",
      "\ttrain_count: 400000, current loss: 0.235252\n",
      "\ttrain_count: 450000, current loss: 0.235027\n",
      "\ttrain_count: 500000, current loss: 0.235062\n",
      "\ttrain_count: 550000, current loss: 0.235109\n",
      "\ttrain_count: 600000, current loss: 0.235113\n",
      "Epoch: 8, train loss: 0.235037, valid loss: 0.281718, time: 0:32:41.853327\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.228917\n",
      "\ttrain_count: 100000, current loss: 0.226492\n",
      "\ttrain_count: 150000, current loss: 0.226967\n",
      "\ttrain_count: 200000, current loss: 0.227197\n",
      "\ttrain_count: 250000, current loss: 0.227067\n",
      "\ttrain_count: 300000, current loss: 0.226602\n",
      "\ttrain_count: 350000, current loss: 0.226523\n",
      "\ttrain_count: 400000, current loss: 0.226844\n",
      "\ttrain_count: 450000, current loss: 0.226638\n",
      "\ttrain_count: 500000, current loss: 0.226704\n",
      "\ttrain_count: 550000, current loss: 0.226773\n",
      "\ttrain_count: 600000, current loss: 0.226805\n",
      "Epoch: 9, train loss: 0.226738, valid loss: 0.279104, time: 0:36:47.402945\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.221082\n",
      "\ttrain_count: 100000, current loss: 0.218634\n",
      "\ttrain_count: 150000, current loss: 0.219125\n",
      "\ttrain_count: 200000, current loss: 0.219348\n",
      "\ttrain_count: 250000, current loss: 0.219249\n",
      "\ttrain_count: 300000, current loss: 0.218773\n",
      "\ttrain_count: 350000, current loss: 0.218675\n",
      "\ttrain_count: 400000, current loss: 0.218983\n",
      "\ttrain_count: 450000, current loss: 0.218786\n",
      "\ttrain_count: 500000, current loss: 0.218872\n",
      "\ttrain_count: 550000, current loss: 0.218980\n",
      "\ttrain_count: 600000, current loss: 0.219031\n",
      "Epoch: 10, train loss: 0.218980, valid loss: 0.277257, time: 0:40:50.837406\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.213820\n",
      "\ttrain_count: 100000, current loss: 0.211372\n",
      "\ttrain_count: 150000, current loss: 0.211840\n",
      "\ttrain_count: 200000, current loss: 0.212047\n",
      "\ttrain_count: 250000, current loss: 0.211966\n",
      "\ttrain_count: 300000, current loss: 0.211524\n",
      "\ttrain_count: 350000, current loss: 0.211435\n",
      "\ttrain_count: 400000, current loss: 0.211748\n",
      "\ttrain_count: 450000, current loss: 0.211565\n",
      "\ttrain_count: 500000, current loss: 0.211681\n",
      "\ttrain_count: 550000, current loss: 0.211813\n",
      "\ttrain_count: 600000, current loss: 0.211884\n",
      "Epoch: 11, train loss: 0.211845, valid loss: 0.275541, time: 0:45:00.940191\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.207065\n",
      "\ttrain_count: 100000, current loss: 0.204680\n",
      "\ttrain_count: 150000, current loss: 0.205138\n",
      "\ttrain_count: 200000, current loss: 0.205379\n",
      "\ttrain_count: 250000, current loss: 0.205320\n",
      "\ttrain_count: 300000, current loss: 0.204890\n",
      "\ttrain_count: 350000, current loss: 0.204788\n",
      "\ttrain_count: 400000, current loss: 0.205093\n",
      "\ttrain_count: 450000, current loss: 0.204910\n",
      "\ttrain_count: 500000, current loss: 0.205034\n",
      "\ttrain_count: 550000, current loss: 0.205182\n",
      "\ttrain_count: 600000, current loss: 0.205274\n",
      "Epoch: 12, train loss: 0.205249, valid loss: 0.274219, time: 0:49:06.790149\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.200740\n",
      "\ttrain_count: 100000, current loss: 0.198428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_count: 150000, current loss: 0.198826\n",
      "\ttrain_count: 200000, current loss: 0.199078\n",
      "\ttrain_count: 250000, current loss: 0.199041\n",
      "\ttrain_count: 300000, current loss: 0.198638\n",
      "\ttrain_count: 350000, current loss: 0.198561\n",
      "\ttrain_count: 400000, current loss: 0.198870\n",
      "\ttrain_count: 450000, current loss: 0.198708\n",
      "\ttrain_count: 500000, current loss: 0.198848\n",
      "\ttrain_count: 550000, current loss: 0.199013\n",
      "\ttrain_count: 600000, current loss: 0.199117\n",
      "Epoch: 13, train loss: 0.199096, valid loss: 0.272720, time: 0:53:16.966923\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.195027\n",
      "\ttrain_count: 100000, current loss: 0.192674\n",
      "\ttrain_count: 150000, current loss: 0.193036\n",
      "\ttrain_count: 200000, current loss: 0.193279\n",
      "\ttrain_count: 250000, current loss: 0.193243\n",
      "\ttrain_count: 300000, current loss: 0.192821\n",
      "\ttrain_count: 350000, current loss: 0.192752\n",
      "\ttrain_count: 400000, current loss: 0.193057\n",
      "\ttrain_count: 450000, current loss: 0.192901\n",
      "\ttrain_count: 500000, current loss: 0.193046\n",
      "\ttrain_count: 550000, current loss: 0.193218\n",
      "\ttrain_count: 600000, current loss: 0.193346\n",
      "Epoch: 14, train loss: 0.193329, valid loss: 0.272305, time: 0:57:27.203774\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.189523\n",
      "\ttrain_count: 100000, current loss: 0.187212\n",
      "\ttrain_count: 150000, current loss: 0.187585\n",
      "\ttrain_count: 200000, current loss: 0.187859\n",
      "\ttrain_count: 250000, current loss: 0.187821\n",
      "\ttrain_count: 300000, current loss: 0.187400\n",
      "\ttrain_count: 350000, current loss: 0.187333\n",
      "\ttrain_count: 400000, current loss: 0.187621\n",
      "\ttrain_count: 450000, current loss: 0.187464\n",
      "\ttrain_count: 500000, current loss: 0.187610\n",
      "\ttrain_count: 550000, current loss: 0.187789\n",
      "\ttrain_count: 600000, current loss: 0.187918\n",
      "Epoch: 15, train loss: 0.187901, valid loss: 0.271496, time: 1:01:37.337340\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.184333\n",
      "\ttrain_count: 100000, current loss: 0.182059\n",
      "\ttrain_count: 150000, current loss: 0.182400\n",
      "\ttrain_count: 200000, current loss: 0.182682\n",
      "\ttrain_count: 250000, current loss: 0.182669\n",
      "\ttrain_count: 300000, current loss: 0.182239\n",
      "\ttrain_count: 350000, current loss: 0.182173\n",
      "\ttrain_count: 400000, current loss: 0.182454\n",
      "\ttrain_count: 450000, current loss: 0.182301\n",
      "\ttrain_count: 500000, current loss: 0.182454\n",
      "\ttrain_count: 550000, current loss: 0.182636\n",
      "\ttrain_count: 600000, current loss: 0.182763\n",
      "Epoch: 16, train loss: 0.182753, valid loss: 0.271156, time: 1:05:46.226165\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.179474\n",
      "\ttrain_count: 100000, current loss: 0.177311\n",
      "\ttrain_count: 150000, current loss: 0.177637\n",
      "\ttrain_count: 200000, current loss: 0.177863\n",
      "\ttrain_count: 250000, current loss: 0.177820\n",
      "\ttrain_count: 300000, current loss: 0.177404\n",
      "\ttrain_count: 350000, current loss: 0.177326\n",
      "\ttrain_count: 400000, current loss: 0.177601\n",
      "\ttrain_count: 450000, current loss: 0.177450\n",
      "\ttrain_count: 500000, current loss: 0.177594\n",
      "\ttrain_count: 550000, current loss: 0.177780\n",
      "\ttrain_count: 600000, current loss: 0.177911\n",
      "Epoch: 17, train loss: 0.177904, valid loss: 0.270537, time: 1:09:36.061221\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.174715\n",
      "\ttrain_count: 100000, current loss: 0.172559\n",
      "\ttrain_count: 150000, current loss: 0.172858\n",
      "\ttrain_count: 200000, current loss: 0.173083\n",
      "\ttrain_count: 250000, current loss: 0.173068\n",
      "\ttrain_count: 300000, current loss: 0.172656\n",
      "\ttrain_count: 350000, current loss: 0.172612\n",
      "\ttrain_count: 400000, current loss: 0.172898\n",
      "\ttrain_count: 450000, current loss: 0.172759\n",
      "\ttrain_count: 500000, current loss: 0.172917\n",
      "\ttrain_count: 550000, current loss: 0.173119\n",
      "\ttrain_count: 600000, current loss: 0.173267\n",
      "Epoch: 18, train loss: 0.173266, valid loss: 0.269885, time: 1:13:44.937059\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.170339\n",
      "\ttrain_count: 100000, current loss: 0.168195\n",
      "\ttrain_count: 150000, current loss: 0.168471\n",
      "\ttrain_count: 200000, current loss: 0.168708\n",
      "\ttrain_count: 250000, current loss: 0.168695\n",
      "\ttrain_count: 300000, current loss: 0.168279\n",
      "\ttrain_count: 350000, current loss: 0.168233\n",
      "\ttrain_count: 400000, current loss: 0.168500\n",
      "\ttrain_count: 450000, current loss: 0.168368\n",
      "\ttrain_count: 500000, current loss: 0.168529\n",
      "\ttrain_count: 550000, current loss: 0.168739\n",
      "\ttrain_count: 600000, current loss: 0.168884\n",
      "Epoch: 19, train loss: 0.168889, valid loss: 0.270137, time: 1:17:51.415159\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.166139\n",
      "\ttrain_count: 100000, current loss: 0.164037\n",
      "\ttrain_count: 150000, current loss: 0.164344\n",
      "\ttrain_count: 200000, current loss: 0.164604\n",
      "\ttrain_count: 250000, current loss: 0.164611\n",
      "\ttrain_count: 300000, current loss: 0.164213\n",
      "\ttrain_count: 350000, current loss: 0.164157\n",
      "\ttrain_count: 400000, current loss: 0.164404\n",
      "\ttrain_count: 450000, current loss: 0.164279\n",
      "\ttrain_count: 500000, current loss: 0.164450\n",
      "\ttrain_count: 550000, current loss: 0.164665\n",
      "\ttrain_count: 600000, current loss: 0.164818\n",
      "Epoch: 20, train loss: 0.164829, valid loss: 0.270250, time: 1:21:52.585027\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.162330\n",
      "\ttrain_count: 100000, current loss: 0.160234\n",
      "\ttrain_count: 150000, current loss: 0.160533\n",
      "\ttrain_count: 200000, current loss: 0.160774\n",
      "\ttrain_count: 250000, current loss: 0.160755\n",
      "\ttrain_count: 300000, current loss: 0.160331\n",
      "\ttrain_count: 350000, current loss: 0.160263\n",
      "\ttrain_count: 400000, current loss: 0.160511\n",
      "\ttrain_count: 450000, current loss: 0.160378\n",
      "\ttrain_count: 500000, current loss: 0.160545\n",
      "\ttrain_count: 550000, current loss: 0.160753\n",
      "\ttrain_count: 600000, current loss: 0.160900\n",
      "Epoch: 21, train loss: 0.160906, valid loss: 0.269703, time: 1:25:47.894690\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.158527\n",
      "\ttrain_count: 100000, current loss: 0.156430\n",
      "\ttrain_count: 150000, current loss: 0.156692\n",
      "\ttrain_count: 200000, current loss: 0.156921\n",
      "\ttrain_count: 250000, current loss: 0.156926\n",
      "\ttrain_count: 300000, current loss: 0.156522\n",
      "\ttrain_count: 350000, current loss: 0.156461\n",
      "\ttrain_count: 400000, current loss: 0.156691\n",
      "\ttrain_count: 450000, current loss: 0.156573\n",
      "\ttrain_count: 500000, current loss: 0.156733\n",
      "\ttrain_count: 550000, current loss: 0.156949\n",
      "\ttrain_count: 600000, current loss: 0.157103\n",
      "Epoch: 22, train loss: 0.157113, valid loss: 0.269669, time: 1:29:44.834156\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.154899\n",
      "\ttrain_count: 100000, current loss: 0.152870\n",
      "\ttrain_count: 150000, current loss: 0.153121\n",
      "\ttrain_count: 200000, current loss: 0.153350\n",
      "\ttrain_count: 250000, current loss: 0.153352\n",
      "\ttrain_count: 300000, current loss: 0.152953\n",
      "\ttrain_count: 350000, current loss: 0.152903\n",
      "\ttrain_count: 400000, current loss: 0.153135\n",
      "\ttrain_count: 450000, current loss: 0.153015\n",
      "\ttrain_count: 500000, current loss: 0.153182\n",
      "\ttrain_count: 550000, current loss: 0.153396\n",
      "\ttrain_count: 600000, current loss: 0.153552\n",
      "Epoch: 23, train loss: 0.153562, valid loss: 0.270258, time: 1:33:44.099122\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.151445\n",
      "\ttrain_count: 100000, current loss: 0.149456\n",
      "\ttrain_count: 150000, current loss: 0.149679\n",
      "\ttrain_count: 200000, current loss: 0.149892\n",
      "\ttrain_count: 250000, current loss: 0.149886\n",
      "\ttrain_count: 300000, current loss: 0.149486\n",
      "\ttrain_count: 350000, current loss: 0.149429\n",
      "\ttrain_count: 400000, current loss: 0.149632\n",
      "\ttrain_count: 450000, current loss: 0.149510\n",
      "\ttrain_count: 500000, current loss: 0.149672\n",
      "\ttrain_count: 550000, current loss: 0.149894\n",
      "\ttrain_count: 600000, current loss: 0.150045\n",
      "Epoch: 24, train loss: 0.150058, valid loss: 0.270163, time: 1:37:35.490965\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.148060\n",
      "\ttrain_count: 100000, current loss: 0.146103\n",
      "\ttrain_count: 150000, current loss: 0.146282\n",
      "\ttrain_count: 200000, current loss: 0.146509\n",
      "\ttrain_count: 250000, current loss: 0.146496\n",
      "\ttrain_count: 300000, current loss: 0.146083\n",
      "\ttrain_count: 350000, current loss: 0.146021\n",
      "\ttrain_count: 400000, current loss: 0.146212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_count: 450000, current loss: 0.146094\n",
      "\ttrain_count: 500000, current loss: 0.146253\n",
      "\ttrain_count: 550000, current loss: 0.146463\n",
      "\ttrain_count: 600000, current loss: 0.146603\n",
      "Epoch: 25, train loss: 0.146612, valid loss: 0.270394, time: 1:41:30.962816\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.144611\n",
      "\ttrain_count: 100000, current loss: 0.142720\n",
      "\ttrain_count: 150000, current loss: 0.142935\n",
      "\ttrain_count: 200000, current loss: 0.143136\n",
      "\ttrain_count: 250000, current loss: 0.143121\n",
      "\ttrain_count: 300000, current loss: 0.142744\n",
      "\ttrain_count: 350000, current loss: 0.142675\n",
      "\ttrain_count: 400000, current loss: 0.142882\n",
      "\ttrain_count: 450000, current loss: 0.142782\n",
      "\ttrain_count: 500000, current loss: 0.142944\n",
      "\ttrain_count: 550000, current loss: 0.143168\n",
      "\ttrain_count: 600000, current loss: 0.143311\n",
      "Epoch: 26, train loss: 0.143324, valid loss: 0.270750, time: 1:45:33.303978\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.141505\n",
      "\ttrain_count: 100000, current loss: 0.139632\n",
      "\ttrain_count: 150000, current loss: 0.139878\n",
      "\ttrain_count: 200000, current loss: 0.140080\n",
      "\ttrain_count: 250000, current loss: 0.140055\n",
      "\ttrain_count: 300000, current loss: 0.139652\n",
      "\ttrain_count: 350000, current loss: 0.139598\n",
      "\ttrain_count: 400000, current loss: 0.139792\n",
      "\ttrain_count: 450000, current loss: 0.139685\n",
      "\ttrain_count: 500000, current loss: 0.139850\n",
      "\ttrain_count: 550000, current loss: 0.140070\n",
      "\ttrain_count: 600000, current loss: 0.140214\n",
      "Epoch: 27, train loss: 0.140232, valid loss: 0.271219, time: 1:49:33.626202\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.138556\n",
      "\ttrain_count: 100000, current loss: 0.136730\n",
      "\ttrain_count: 150000, current loss: 0.136966\n",
      "\ttrain_count: 200000, current loss: 0.137143\n",
      "\ttrain_count: 250000, current loss: 0.137117\n",
      "\ttrain_count: 300000, current loss: 0.136718\n",
      "\ttrain_count: 350000, current loss: 0.136657\n",
      "\ttrain_count: 400000, current loss: 0.136844\n",
      "\ttrain_count: 450000, current loss: 0.136738\n",
      "\ttrain_count: 500000, current loss: 0.136893\n",
      "\ttrain_count: 550000, current loss: 0.137101\n",
      "\ttrain_count: 600000, current loss: 0.137242\n",
      "Epoch: 28, train loss: 0.137259, valid loss: 0.271710, time: 1:53:34.117538\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.135660\n",
      "\ttrain_count: 100000, current loss: 0.133884\n",
      "\ttrain_count: 150000, current loss: 0.134102\n",
      "\ttrain_count: 200000, current loss: 0.134276\n",
      "\ttrain_count: 250000, current loss: 0.134252\n",
      "\ttrain_count: 300000, current loss: 0.133872\n",
      "\ttrain_count: 350000, current loss: 0.133807\n",
      "\ttrain_count: 400000, current loss: 0.133994\n",
      "\ttrain_count: 450000, current loss: 0.133895\n",
      "\ttrain_count: 500000, current loss: 0.134053\n",
      "\ttrain_count: 550000, current loss: 0.134258\n",
      "\ttrain_count: 600000, current loss: 0.134414\n",
      "Epoch: 29, train loss: 0.134433, valid loss: 0.271492, time: 1:57:33.730566\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.132900\n",
      "\ttrain_count: 100000, current loss: 0.131154\n",
      "\ttrain_count: 150000, current loss: 0.131394\n",
      "\ttrain_count: 200000, current loss: 0.131572\n",
      "\ttrain_count: 250000, current loss: 0.131533\n",
      "\ttrain_count: 300000, current loss: 0.131142\n",
      "\ttrain_count: 350000, current loss: 0.131096\n",
      "\ttrain_count: 400000, current loss: 0.131282\n",
      "\ttrain_count: 450000, current loss: 0.131194\n",
      "\ttrain_count: 500000, current loss: 0.131343\n",
      "\ttrain_count: 550000, current loss: 0.131550\n",
      "\ttrain_count: 600000, current loss: 0.131687\n",
      "Epoch: 30, train loss: 0.131710, valid loss: 0.272483, time: 2:01:33.904566\n",
      "0.26966944876645255\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt,pow\n",
    "import itertools\n",
    "import math\n",
    "from random import random,shuffle,uniform,seed\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "seed(1024)\n",
    "\n",
    "def data_generator(path,no_norm=False,task='c'):\n",
    "    data = open(path,'r')\n",
    "    for row in data:\n",
    "        row = row.strip().split(\" \")\n",
    "        y = float(row[0])\n",
    "        row = row[1:]\n",
    "        x = []\n",
    "        for feature in row:\n",
    "            feature = feature.split(\":\")\n",
    "            idx = int(feature[0])\n",
    "            value = float(feature[1])\n",
    "            x.append([idx,value])\n",
    "\n",
    "        if not no_norm:\n",
    "            r = 0.0\n",
    "            for i in range(len(x)):\n",
    "                r+=x[i][1]*x[i][1]\n",
    "            for i in range(len(x)):\n",
    "                x[i][1] /=r\n",
    "        # if task=='c':\n",
    "        #     if y ==0.0:\n",
    "        #         y = -1.0\n",
    "\n",
    "        yield x,y\n",
    "\n",
    "\n",
    "def dot(u,v):\n",
    "    u_v = 0.\n",
    "    len_u = len(u)\n",
    "    for idx in range(len_u):\n",
    "        uu = u[idx]\n",
    "        vv = v[idx]\n",
    "        u_v+=uu*vv\n",
    "    return u_v\n",
    "\n",
    "def mse_loss_function(y,p):\n",
    "    return (y - p)**2\n",
    "\n",
    "def mae_loss_function(y,p):\n",
    "    y = exp(y)\n",
    "    p = exp(p)\n",
    "    return abs(y - p)\n",
    "\n",
    "def log_loss_function(y,p):\n",
    "    return -(y*log(p)+(1-y)*log(1-p))\n",
    "\n",
    "def exponential_loss_function(y,p):\n",
    "    return log(1+exp(-y*p))\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1/(1+exp(-inX))\n",
    "\n",
    "def bounded_sigmoid(inX):\n",
    "    return 1. / (1. + exp(-max(min(inX, 35.), -35.)))\n",
    "\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self,lr=0.001,momentum=0.9,nesterov=True,adam=False,l2=0.0,l2_fm=0.0,l2_bias=0.0,ini_stdev= 0.01,dropout=0.5,task='c',n_components=4,nb_epoch=5,interaction=False,no_norm=False):\n",
    "        self.W = []\n",
    "        self.V = []        \n",
    "        self.bias = uniform(-ini_stdev, ini_stdev)\n",
    "        self.n_components=n_components\n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "        self.l2_fm = l2_fm\n",
    "        self.l2_bias = l2_bias\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.adam = adam\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.ini_stdev = ini_stdev\n",
    "        self.task = task\n",
    "        self.interaction = interaction\n",
    "        self.dropout = dropout\n",
    "        self.no_norm = no_norm\n",
    "        if self.task!='c':\n",
    "            # self.loss_function = mse_loss_function\n",
    "            self.loss_function = mae_loss_function\n",
    "        else:\n",
    "            # self.loss_function = exponential_loss_function\n",
    "            self.loss_function = log_loss_function\n",
    "\n",
    "    def preload(self,train,test):\n",
    "        train = data_generator(train,self.no_norm,self.task)\n",
    "        dim = 0\n",
    "        count = 0\n",
    "        for x,y in train:\n",
    "            for i in x:\n",
    "                idx,value = i\n",
    "                if idx >dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Training samples:',count)\n",
    "        test = data_generator(test,self.no_norm,self.task)\n",
    "        count=0\n",
    "        for x,y in test:\n",
    "            for i in x:\n",
    "                idx,value = i\n",
    "                if idx >dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Testing samples:',count)\n",
    "        \n",
    "        dim = dim+1\n",
    "        print(\"Number of features:\",dim)\n",
    "        \n",
    "        self.W = [uniform(-self.ini_stdev, self.ini_stdev) for _ in range(dim)]\n",
    "        self.Velocity_W = [0.0 for _ in range(dim)]\n",
    "        \n",
    "        \n",
    "        self.V = [[uniform(-self.ini_stdev, self.ini_stdev) for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.Velocity_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        \n",
    "        self.Velocity_bias = 0.0\n",
    "        \n",
    "        self.dim = dim\n",
    "        \n",
    "        \n",
    "    def adam_init(self):\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.decay = 0.\n",
    "        self.inital_decay = self.decay \n",
    "\n",
    "        dim =self.dim\n",
    "\n",
    "        self.m_W = [0.0 for _ in range(dim)]\n",
    "        self.v_W = [0.0 for _ in range(dim)]\n",
    "\n",
    "        self.m_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.v_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "\n",
    "        self.m_bias = 0.0\n",
    "        self.v_bias = 0.0\n",
    "\n",
    "\n",
    "    def adam_update(self,lr,x,residual):\n",
    "\n",
    "        if 0.<self.dropout<1.:\n",
    "            self.droupout_x(x)\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.inital_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "\n",
    "        lr_t = lr * sqrt(1. - pow(self.beta_2, t)) / (1. - pow(self.beta_1, t))\n",
    "        \n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            g = residual*value\n",
    "\n",
    "            m = self.m_W[idx]\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "            v = self.v_W[idx]\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "            p = self.W[idx]\n",
    "            p_t = p - lr_t *m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            if self.l2>0:\n",
    "                p_t = p_t - lr_t*self.l2*p\n",
    "\n",
    "            self.m_W[idx] = m_t\n",
    "            self.v_W[idx] = v_t\n",
    "            self.W[idx] = p_t\n",
    "\n",
    "        if self.interaction:\n",
    "            self._adam_update_fm(lr_t,x,residual)\n",
    "\n",
    "\n",
    "        m = self.m_bias\n",
    "        m_t = (self.beta_1 * m) + (1. - self.beta_1)*residual\n",
    "\n",
    "        v = self.v_bias\n",
    "        v_t = (self.beta_2 * v) + (1. - self.beta_2)*(residual**2)\n",
    "\n",
    "        p = self.bias\n",
    "        p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "        if self.l2_bias>0:\n",
    "            pt = pt - lr_t * self.l2_bias*p\n",
    "\n",
    "        self.m_bias = m_t\n",
    "        self.v_bias = v_t\n",
    "        self.bias = p_t\n",
    "\n",
    "        self.iterations+=1\n",
    "\n",
    "    def _adam_update_fm(self,lr_t,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                v = self.V[idx_i][f]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                g = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "\n",
    "                m = self.m_V[idx_i][f]\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "                v = self.v_V[idx_i][f]\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "                p = self.V[idx_i][f]\n",
    "                p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                if self.l2_fm>0:\n",
    "                    p_t = p_t - lr_t * self.l2_fm*p\n",
    "\n",
    "                self.m_V[idx_i][f] = m_t\n",
    "                self.v_V[idx_i][f] = v_t\n",
    "                self.V[idx_i][f] = p_t\n",
    "\n",
    "    def droupout_x(self,x):\n",
    "        new_x = []\n",
    "        for i, var in enumerate(x):\n",
    "            if random() > self.dropout:\n",
    "                del x[i]\n",
    "\n",
    "    def _predict_fm(self,x):\n",
    "        len_x = len(x)\n",
    "        n_components = self.n_components\n",
    "        pred = 0.0\n",
    "        self.sum_f_dict = {}\n",
    "        for f in range(n_components):\n",
    "            sum_f = 0.0\n",
    "            sum_sqr_f = 0.0\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                d = self.V[idx_i][f] * value_i\n",
    "                sum_f +=d\n",
    "                sum_sqr_f +=d*d\n",
    "            pred+= 0.5 * (sum_f*sum_f - sum_sqr_f);\n",
    "            self.sum_f_dict[f] = sum_f\n",
    "        return pred\n",
    "\n",
    "    def _predict_one(self,x):\n",
    "        pred = self.bias\n",
    "        # pred = 0.0\n",
    "        for idx,value in x:\n",
    "            pred+=self.W[idx]*value\n",
    "        \n",
    "        if self.interaction:\n",
    "            pred+=self._predict_fm(x)\n",
    "\n",
    "        if self.task=='c':\n",
    "            pred = bounded_sigmoid(pred)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _update_fm(self,lr,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                v = self.V[idx_i][f]\n",
    "                grad = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "                \n",
    "                self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                if self.nesterov:\n",
    "                    self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                self.V[idx_i][f] = self.V[idx_i][f] + self.Velocity_V[idx_i][f] - lr*self.l2_fm*self.V[idx_i][f]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self,lr,x,residual):\n",
    "\n",
    "        if 0.<self.dropout<1.:\n",
    "            self.droupout_x(x)\n",
    "\n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            grad = residual*value\n",
    "            self.Velocity_W[idx] =  self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            if self.nesterov:\n",
    "                 self.Velocity_W[idx] = self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            self.W[idx] = self.W[idx] + self.Velocity_W[idx] - lr*self.l2*self.W[idx]\n",
    "            \n",
    "        if self.interaction:\n",
    "            self._update_fm(lr,x,residual)\n",
    "\n",
    "        self.Velocity_bias = self.momentum*self.Velocity_bias - lr*residual\n",
    "        if self.nesterov:\n",
    "            self.Velocity_bias = self.momentum*self.Velocity_bias - lr*residual\n",
    "        self.bias = self.bias +self.Velocity_bias - lr*self.l2_bias*self.bias\n",
    "\n",
    "    def predict(self,path,out):\n",
    "\n",
    "        data = data_generator(path,self.no_norm,self.task)\n",
    "        y_preds =[]\n",
    "        with open(out, 'w') as outfile:\n",
    "            ID = 0\n",
    "            outfile.write('%s,%s\\n' % ('test_id', 'is_duplicate'))\n",
    "            for d in data:\n",
    "                x,y = d\n",
    "                p = self._predict_one(x)\n",
    "                outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "                ID+=1\n",
    "\n",
    "\n",
    "    def validate(self,path):\n",
    "        data = data_generator(path,self.no_norm,self.task)\n",
    "        loss = 0.0\n",
    "        count = 0.0\n",
    "\n",
    "        for d in data:\n",
    "            x,y = d\n",
    "            p = self._predict_one(x)\n",
    "            loss+=self.loss_function(y,p)\n",
    "            count+=1\n",
    "        return loss/count\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = []\n",
    "        weights.append(self.W)\n",
    "        weights.append(self.V)\n",
    "        weights.append(self.bias)\n",
    "        # weights.append(self.Velocity_W)\n",
    "        # weights.append(self.Velocity_V)\n",
    "        weights.append(self.dim)\n",
    "        pickle.dump(weights,open('sgd_fm.pkl','wb'))\n",
    "\n",
    "    def load_weights(self):\n",
    "        weights = pickle.load(open('sgd_fm.pkl','rb'))\n",
    "        self.W = weights[0]\n",
    "        self.V = weights[1]\n",
    "        self.bias = weights[2]\n",
    "        # self.Velocity_W = weights[3]\n",
    "        # self.Velocity_V = weights[4]\n",
    "        self.dim = weights[3]\n",
    "        \n",
    "\n",
    "    def train(self,path,valid_path = None,in_memory=False):\n",
    "\n",
    "        start = datetime.now()\n",
    "        lr = self.lr\n",
    "        if self.adam:\n",
    "            self.adam_init()\n",
    "            self.update = self.adam_update\n",
    "\n",
    "        if in_memory:\n",
    "            data = data_generator(path,self.no_norm,self.task)\n",
    "            data = [d for d in data]\n",
    "        best_loss = 999999\n",
    "        best_epoch = 0\n",
    "        for epoch in range(1,self.nb_epoch+1):\n",
    "            if not in_memory:\n",
    "                data = data_generator(path,self.no_norm,self.task)\n",
    "            train_loss = 0.0\n",
    "            train_count = 0\n",
    "            for x,y in data:\n",
    "                p = self._predict_one(x)\n",
    "                if self.task!='c':                    \n",
    "                    residual = -(y-p)\n",
    "                else:\n",
    "                    # residual = -y*(1.0-1.0/(1.0+exp(-y*p)));\n",
    "                    residual = -(y-p)\n",
    "\n",
    "                self.update(lr,x,residual)\n",
    "                if train_count%50000==0:\n",
    "                    if train_count ==0:\n",
    "                        print('\\ttrain_count: %s, current loss: %.6f'%(train_count,0.0))\n",
    "                    else:\n",
    "                        print('\\ttrain_count: %s, current loss: %.6f'%(train_count,train_loss/train_count))\n",
    "\n",
    "                train_loss += self.loss_function(y,p)\n",
    "                train_count += 1\n",
    "\n",
    "            epoch_end = datetime.now()\n",
    "            duration = epoch_end-start\n",
    "            \n",
    "            if valid_path:\n",
    "                valid_loss = self.validate(valid_path)\n",
    "                print('Epoch: %s, train loss: %.6f, valid loss: %.6f, time: %s'%(epoch,train_loss/train_count,valid_loss,duration))\n",
    "                if valid_loss<best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    self.save_weights()\n",
    "                    print('save_weights')\n",
    "            else:\n",
    "                print('Epoch: %s, train loss: %.6f, time: %s'%(epoch,train_loss/train_count,duration))\n",
    "\n",
    "\n",
    "path = \"DATA/\"\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.001,adam=True,dropout=0.8,l2=0.00,l2_fm=0.00,task='c',n_components=1,nb_epoch=30,interaction=True,no_norm=False)\n",
    "sgd.preload(path+'X_tfidf.svm',path+'X_t_tfidf.svm')\n",
    "# sgd.load_weights()\n",
    "sgd.train(path+'X_train_tfidf.svm',path+'X_test_tfidf.svm',in_memory=False)\n",
    "sgd.load_weights()\n",
    "sgd.predict(path+'X_test_tfidf.svm',out='valid.csv')\n",
    "print(sgd.validate(path+'X_test_tfidf.svm'))\n",
    "sgd.predict(path+'X_t_tfidf.svm',out='out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
